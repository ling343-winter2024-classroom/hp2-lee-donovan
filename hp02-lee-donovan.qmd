---
title: "Homework Project 02"
author: "Donovan Lee"
date: 04-23-2024
format:
  html:
    code-fold: true
    embed-resources: true
---
##


## Set-Up
```{r}
#| warning: false
#| message: false

library(tidyverse)
library(tidytext)
library(here)
library(gt)
```


## What's This Data?
This data was obtained from the [subjectivity dataset v1.0](https://www.cs.cornell.edu/people/pabo/movie-review-data/). From this download, we are given a zipped file, rotten_imbd which contains three files: subjdata.README.1.0, plot.tok.gt9.5000, and quote.tok.gt9.5000.\n\n

```{r}
#| echo: false

tibble::tribble(
  ~file, ~summary,
  "subjdata.README.1.0", "a file that contains information about what is in this data and how it was obtained and processed",
  "plot.tok.gt9.5000", "a file that contains 5000 lines of plot summaries derived from IMDb",
  "quote.tok.gt9.5000", "a file that contains 5000 lines of movie reviews derived from RottenTomatoes") |>
  gt() |>
  opt_row_striping() |>
  opt_stylize(style = 6, color = "red", add_row_striping = TRUE)
```

From the README, the two files are separated into two categories: objective and subjective. The plot file is labeled as the objective category and the quote file is labeled as the subjective category.


## Importing the Data
In order to analyze this dataset, we have to import the files into a format that allows us to easily modify it.
```{r}
#| layout-nrow: 2

obj_data <- read_tsv(here("rotten_imdb/plot.tok.gt9.5000"), col_names = "sentence", show_col_types = FALSE) |>
  mutate(sent_id = c(1:5000), .before = sentence)
subj_data <- read_tsv(here("rotten_imdb/quote.tok.gt9.5000"), col_names = "sentence", show_col_types = FALSE) |>
  mutate(sent_id = c(1:5000), .before = sentence)
obj_data
subj_data
```
Here, we are able to create a dataframe that contains two variables: sent_id for the sentence number and sentence for the actual sentence content.\n\n

## Time for Analysis!
Since we are looking at a large swath of words, we can attempt to do some analysis using tokens. But first, we will have to tokenize the sentences:
```{r}
#| layout-nrow: 2

obj_tokens <- obj_data  |>
  unnest_tokens(word, sentence)
subj_tokens <- subj_data |>
  unnest_tokens(word, sentence)
obj_tokens
subj_tokens
```
We can see that there are `r nrow(obj_tokens)` tokens in our objective dataset and `r nrow(subj_tokens)` tokens in our subjective dataset; a difference of `r nrow(obj_tokens) - nrow(subj_tokens)` tokens. However, this doesn't really tell us anything about how different the two datasets since it may be likely that there are a lot of words such as "the", "a", and "of" that make up the majority of these datasets' tokens, so we will remove these words (known as stopwords) from our datasets.
```{r}
#| layout-nrow: 2

obj_tokens <- obj_tokens |>
  anti_join(stop_words, join_by(word == word))
subj_tokens <- subj_tokens |>
  anti_join(stop_words, join_by(word == word))
obj_tokens
subj_tokens
```
Now there are `r nrow(obj_tokens)` tokens in the objective dataset and `r nrow(subj_tokens)` tokens in the subjective dataset, and we can see how almost half of the original dataset was filled with those stopwords. Although we can see the amount of the more important tokens in each of the datasets, we still don't know what this really entails.

Perhaps if we divide this large count into smaller counts, we can get a better idea of what it means to be objective and subjective when talking about films. We can count up each unique token in each dataset and see the tokens that appear the most times:

### Table Visual
```{r}
#| layout-ncol: 2

obj_tokens |>
  count(word, sort = TRUE) |>
  slice(1:15) |>
  gt()  |>
  opt_row_striping() |>
  opt_stylize(style = 6, color = "red", add_row_striping = TRUE) |>
  tab_header(
    title = "Objective",
    subtitle = "Top 15 Tokens"
  )
subj_tokens |>
  count(word, sort = TRUE) |>
  slice(1:15) |>
  gt()  |>
  opt_row_striping() |>
  opt_stylize(style = 6, color = "red", add_row_striping = TRUE) |>
  tab_header(
    title = "Subjective",
    subtitle = "Top 15 Tokens"
  )
```
### Graph Visual
```{r}
#| layout-nrow: 2
obj_tokens |>
  count(word, sort = TRUE) |>
  slice(1:15) |>
  ggplot(aes(x = n, y = reorder(word, n))) +
  geom_col(fill = "darkgreen") +
  labs(
    title = "Top 15 Objective Words",
    x = "# of Appearances",
    y = "Word"
  )
subj_tokens |>
  count(word, sort = TRUE) |>
  slice(1:15) |>
  ggplot(aes(x = n, y = reorder(word, n))) +
  geom_col(fill = "gold") +
  labs(
    title = "Top 15 Subjective Words",
    x = "# of Appearances",
    y = "Word"
  )
```
Looking at these visuals, we can now see some tangible information about these two datasets. In the objective dataset, nothing appears to be off the mark: life, story, world, and film are words that would be commonly found in sentences that discuss a film's plot. On the otherhand, the subjective dataset gleans some interesting things. For one thing, we can see that the words "film" and "movie" outnumber the rest of the word with counts of 740 and 642, when the third most common word "story" appears 234 times. Speaking of the most common words in the subjective dataset, since it was derived from film reviews, it would make sense that "film", "movie", and "story" are the most common: the reviews are talking about the films themselves than giving a plot summary.

We can also attempt a sentiment analysis for each of the dataset. For this analysis, we will use Bing Liu's sentiment evaluations.

### Table Visual
```{r}
#| layout-ncol: 4

obj_ments <- obj_tokens |>
  inner_join(get_sentiments("bing"), join_by(word == word)) |>
  count(word, sentiment, sort = TRUE) |>
  group_by(sentiment) |>
  slice(1:15) |>
  ungroup() |>
  mutate(sentiment = as.factor(sentiment)) |>
  split(~sentiment)
obj_ments$negative |>
  select(!sentiment) |>
  gt() |>
  opt_row_striping() |>
  opt_stylize(style = 6, color = "pink", add_row_striping = TRUE) |>
  tab_header(
    title = "Objective",
    subtitle = "Top 15 Negative Words"
  )
obj_ments$positive |>
  select(!sentiment) |>
  gt() |>
  opt_row_striping() |>
  opt_stylize(style = 6, color = "cyan", add_row_striping = TRUE) |>
  tab_header(
    title = "Objective",
    subtitle = "Top 15 Positive Words"
  )

subj_ments <- subj_tokens |>
  inner_join(get_sentiments("bing"), join_by(word == word)) |>
  count(word, sentiment, sort = TRUE) |>
  group_by(sentiment) |>
  slice(1:15) |>
  ungroup() |>
  mutate(sentiment = as.factor(sentiment)) |>
  split(~sentiment)
subj_ments$negative |>
  select(!sentiment) |>
  gt() |>
  opt_row_striping() |>
  opt_stylize(style = 6, color = "pink", add_row_striping = TRUE) |>
  tab_header(
    title = "Subjective",
    subtitle = "Top 15 Negative Words"
  )
subj_ments$positive |>
  select(!sentiment) |>
  gt() |>
  opt_row_striping() |>
  opt_stylize(style = 6, color = "cyan", add_row_striping = TRUE) |>
  tab_header(
    title = "Subjective",
    subtitle = "Top 15 Positive Words"
  )
```
### Graph Visual
```{r}
#| layout-nrow: 2

obj_tokens |>
  inner_join(get_sentiments("bing"), join_by(word == word)) |>
  count(word, sentiment, sort = TRUE) |>
  group_by(sentiment) |>
  slice(1:15) |>
  ungroup() |>
  ggplot(aes(x = n, y = reorder(word, n), fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(
    title = "Top 15 Positive/Negative Objective Words",
    x = "# of Appearances",
    y = "Word"
  )
subj_tokens |>
  inner_join(get_sentiments("bing"), join_by(word == word)) |>
  count(word, sentiment, sort = TRUE) |>
  group_by(sentiment) |>
  slice(1:15) |>
  ungroup() |>
  ggplot(aes(x = n, y = reorder(word, n), fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(
    title = "Top 15 Positive/Negative Subjective Words",
    x = "# of Appearances",
    y = "Word"
  )
```
Looking at the objective dataset visuals, we can see a clear outlier with the word "love" as it easily outnumbers the rest of the positive words, and speaking of the positive words in this dataset, we can see some odd "positive" words: "leads", "lead", and "led". These are less likely to be the definition akin to guiding and more so the idea of a "leading" role. There appears to be no oddities present within in the negative words.

Now looking at the subjective dataset visuals, there are clear oddities present in the negative words; "funny" and "plot" are considered to be negative and are within the Top 3. It can be said that "funny" is synonymous with words such as "humor" and "entertaining", which are present in the positive words. Speaking of the positive words, we can also see that the word "love" is the most common positive word. Comparing the two datasets, we can say that "love" in the objective dataset is talking more about film plots with romantic elements ("Romeo loves Juliet") and "love" in the subjective dataset is a reviewer praising the film ("I love how...").

All of our analysis so far as been dealing with single tokens, and this could cause some context to be lost. We can attempt to utilize a bigram model for each of the datasets to add more context to each of the words. We will also remove any stopwords present in these bigrams as they tend to be non-relevant and fill up the dataframe. After cleaning, we will then see the most frequent bigrams present in both datasets:
```{r}
obj_bigram <- obj_data |>
  unnest_ngrams(output = bigram, input = sentence, n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  count(word1, word2, sort = TRUE)
  
subj_bigram <- subj_data |>
  unnest_ngrams(output = bigram, input = sentence, n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  count(word1, word2, sort = TRUE)
```
### Table Visual
```{r}
#| layout-ncol: 2

obj_bigram |>
  slice(1:15) |>
  gt()  |>
  opt_row_striping() |>
  opt_stylize(style = 6, color = "red", add_row_striping = TRUE) |>
  tab_header(
    title = "Objective",
    subtitle = "Top 15 Bigrams"
  )
subj_bigram |>
  slice(1:15) |>
  gt()  |>
  opt_row_striping() |>
  opt_stylize(style = 6, color = "red", add_row_striping = TRUE) |>
  tab_header(
    title = "Subjective",
    subtitle = "Top 15 Bigrams"
  )
```
### Graph Visual
```{r}
obj_bigram |>
  unite(col = "bigram", c("word1", "word2"), sep = " ") |>
  slice(1:15) |>
  ggplot(aes(x = n, y = reorder(bigram, n))) +
  geom_col(fill = "darkgreen") +
  labs(
    title = "Top 15 Objective Bigrams",
    x = "# of Appearances",
    y = "Bigram"
  )
subj_bigram |>
  unite(col = "bigram", c("word1", "word2"), sep = " ") |>
  slice(1:15) |>
  ggplot(aes(x = n, y = reorder(bigram, n))) +
  geom_col(fill = "gold") +
  labs(
    title = "Top 15 Subjective Bigrams",
    x = "# of Appearances",
    y = "Bigram"
  )
```


So far, we've been looking at these datasets separately, so why don't we try and combine the two into one. Using this newly combined dataset, we can find the tf-idf (term frequency - inverse document frequency) for each word, and this will tell us some information about the important words that are relevant to this dataset.
```{r}
#| message: false

total_tokens <- obj_tokens |>
  mutate(group = "objective") |>
  full_join(mutate(subj_tokens, group = "subjective"))

total_tokens |>
  mutate(group = as.factor(group)) |>
  group_by(group) |>
  count(word, sort = TRUE) |>
  bind_tf_idf(word, group, n) |>
  arrange(desc(tf_idf)) |>
  slice(1:15) |>
  ggplot(aes(x = tf_idf, y = reorder(word, tf_idf), fill = group)) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~group, scales = "free_y") +
  scale_fill_manual(values = c("darkgreen", "gold")) +
  labs(
    title = "The Most Important Words Based on tf-idf",
    x = "Word",
    y = "tf-idf",
  )
```
We will first explore the objective set of words. According to this graph, we see that "discovers" is the most important word by far in this dataset, and I believe that this make sense as many films are about the main characters experiencing something new, or discovering. We can also see some names such as "rachel", "matt", and "patricia", and this make sense since a lot of plot synopsis include the characters' names (in this case, these names perhaps appear the most). There are also similar words like "government", "security", and "fbi", all of each can describe a common film trope dealing with conspiracy or something grand in the social world. Not all appears to be right though as the numbers "233" and "38" appear in this set, but it could be caused by the idea that these numbers appear the most frequently in all of the film plots.

Now exploring the subjective set of words, no word appears to be a clear outlier in importance. We can see a lot of adjectives present in this set such as  "predictable", "enjoyable", "worthy", "dumb", and "pretentious", and since all of the subjective dataset comes from film reviews, it would make sense that adjectives would have a greater importance in the dataset. This explanation can be applied for "movie's" since many critics are likely to discuss the things that a film possesses. We can also see a lot of words relating to humor: "laughs", "laugh", and "gags", things that can be associated with reaction, so these would make sense as important words in a film review.


## Some Final Thoughts
Throughout this analysis, we have found the number of tokens in each dataset, seen the most frequent words, evaluated the sentiments of the dataset, looked at the bigrams, and explained the important words for each dataset. There clearly were differences between the two dataset and their word choices.

##